---
title: OpenTelemetry Integration
---

AI agents create unpredictable usage patterns and complex request flows that are hard to monitor with traditional methods. The Apollo MCP Server's OpenTelemetry integration provides the visibility you need to run a reliable service for AI agents.

## **What You Can Monitor**

- **Agent behavior**: Which tools and operations are used most frequently
- **Performance**: Response times and bottlenecks across tool executions and GraphQL operations  
- **Reliability**: Error rates, failed operations, and request success patterns
- **Distributed request flows**: Complete traces from agent request through your Apollo Router and subgraphs, with automatic trace context propagation

## **How It Works**

The server exports metrics, traces, and events using the OpenTelemetry Protocol (OTLP), ensuring compatibility with your existing observability stack and seamless integration with other instrumented Apollo services.

## Usage Guide

### **Quick Start: Local Development**

The fastest way to see Apollo MCP Server telemetry in action is with a local setup that requires only Docker.

**ðŸš€ 5-minute setup:**
1. **Start local observability stack**: `docker run -p 3000:3000 -p 4317:4317 -p 4318:4318 --rm -ti grafana/otel-lgtm`
2. **Add telemetry config** to your `config.yaml`:
   ```yaml
   telemetry:
     exporters:
       metrics:
         otlp:
           endpoint: "http://localhost:4318/v1/metrics"
           protocol: "http/protobuf"
       tracing:
         otlp:
           endpoint: "http://localhost:4318/v1/traces" 
           protocol: "http/protobuf"
   ```
3. **Restart your MCP server** with the updated config
4. **Open Grafana** at http://localhost:3000 (admin/admin) and explore your telemetry data

For detailed steps and dashboard examples, see the [complete Grafana setup guide](guides/telemetry-grafana.mdx).

### **Production Deployment**

For production environments, configure your MCP server to send telemetry to any OTLP-compatible backend. The Apollo MCP Server uses standard OpenTelemetry protocols, ensuring compatibility with all major observability platforms.

#### **Configuration Example**

```yaml
telemetry:
  service_name: "mcp-server-prod"      # Custom service name
  exporters:
    metrics:
      otlp:
        endpoint: "https://your-metrics-endpoint"
        protocol: "http/protobuf"       # or "grpc"
    tracing:
      otlp:
        endpoint: "https://your-traces-endpoint" 
        protocol: "http/protobuf"
```

#### **Observability Platform Integration**

The MCP server works with any OTLP-compatible backend. Consult your provider's documentation for specific endpoint URLs and authentication:

- **[Datadog OTLP Integration](https://docs.datadoghq.com/tracing/setup_overview/open_standards/otlp_ingest_in_datadog/)** - Native OTLP support
- **[New Relic OpenTelemetry](https://docs.newrelic.com/docs/more-integrations/open-source-telemetry-integrations/opentelemetry/)** - Direct OTLP ingestion
- **[AWS Observability](https://aws-otel.github.io/docs/introduction)** - Via AWS Distro for OpenTelemetry
- **[Grafana Cloud](https://grafana.com/docs/grafana-cloud/send-data/otlp/)** - Hosted Grafana with OTLP
- **[Honeycomb](https://docs.honeycomb.io/getting-data-in/opentelemetry/)** - OpenTelemetry-native platform
- **[Jaeger](https://www.jaegertracing.io/docs/1.50/deployment/)** - Self-hosted tracing
- **[OpenTelemetry Collector](https://opentelemetry.io/docs/collector/deployment/)** - Self-hosted with flexible routing

#### **Production Configuration Best Practices**

**Environment and Security:**
```yaml
# Set via environment variable
export ENVIRONMENT=production

telemetry:
  service_name: "apollo-mcp-server"
  version: "1.0.0"                     # Version for correlation
  exporters:
    metrics:
      otlp:
        endpoint: "https://secure-endpoint"  # Always use HTTPS
        protocol: "http/protobuf"           # Generally more reliable than gRPC
```

**Performance Considerations:**
- **Protocol choice**: `http/protobuf` is often more reliable through firewalls and load balancers than `grpc`
- **Batch export**: OpenTelemetry automatically batches telemetry data for efficiency
- **Network timeouts**: Default timeouts are usually appropriate, but monitor for network issues

**Resource Correlation:**
- The `ENVIRONMENT` variable automatically tags all telemetry with `deployment.environment.name`
- Use consistent `service_name` across all your Apollo infrastructure (Router, subgraphs, MCP server)
- Set `version` to track releases and correlate issues with deployments

#### **Troubleshooting**

**Common Issues:**
- **Connection refused**: Verify endpoint URL and network connectivity
- **Authentication errors**: Check if your provider requires API keys or special headers
- **Missing data**: Confirm your observability platform supports OTLP and is configured to receive data
- **High memory usage**: Monitor telemetry export frequency and consider sampling for high-volume environments

**Verification:**
```bash
# Check if telemetry is being exported (look for connection attempts)
curl -v https://your-endpoint/v1/metrics

# Monitor server logs for OpenTelemetry export errors
./apollo-mcp-server --config config.yaml 2>&1 | grep -i "otel\|telemetry"
```

## Configuration Reference

The OpenTelemetry integration is configured via the `telemetry` section in your config.yaml file.

### **Basic Configuration**

| Parameter | Type | Description | Default |
| :--- | :--- | :--- | :--- |
| service_name | String | The service name in telemetry data. | `apollo-mcp-server` |
| version | String | The service version in telemetry data. | Current crate version |
| exporters | Object | Configuration for telemetry exporters. | `null` (no telemetry) |

### **Environment Variables**

| Variable | Description | Default |
| :--- | :--- | :--- |
| `ENVIRONMENT` | Sets the deployment environment for telemetry resource attributes. | `development` |

### **Automatic Resource Attributes**

The server automatically sets these OpenTelemetry resource attributes:

| Attribute | Value | Purpose |
| :--- | :--- | :--- |
| `service.name` | From `service_name` config | Service identification |
| `service.version` | From `version` config | Service version tracking |
| `deployment.environment.name` | From `ENVIRONMENT` variable | Environment correlation (dev, staging, prod) |

### **Exporters Configuration**

`exporters`

| Parameter | Type | Description |
| :--- | :--- | :--- |
| metrics | Object | Configuration for metrics exporters. |
| tracing | Object | Configuration for tracing exporters. |

`metrics.otlp` / `tracing.otlp`

| Parameter | Type | Description | Default |
| :--- | :--- | :--- | :--- |
| endpoint | String | The OTLP endpoint to send data to. | `http://localhost:4317` |
| protocol | String | The OTLP protocol to use. Supported values are `grpc` and `http/protobuf`. | `grpc` |


## Emitted Metrics

The server emits the following custom metrics, which are invaluable for monitoring and alerting. All duration metrics are in milliseconds.

| Metric Name | Type | Description | Attributes |
|---|---|---|---|
| `apollo.mcp.initialize.count` | Counter | Incremented for each `initialize` request. | (none) |
| `apollo.mcp.list_tools.count` | Counter | Incremented for each `list_tools` request. | (none) |
| `apollo.mcp.get_info.count` | Counter | Incremented for each `get_info` request. | (none) |
| `apollo.mcp.tool.count` | Counter | Incremented for each tool call. | `tool_name`, `success` (bool) |
| `apollo.mcp.tool.duration` | Histogram | Measures the execution duration of each tool call. | `tool_name`, `success` (bool) |
| `apollo.mcp.operation.count`| Counter | Incremented for each downstream GraphQL operation executed by a tool. | `operation.id`, `operation.type` ("persisted_query" or "operation"), `success` (bool) |
| `apollo.mcp.operation.duration`| Histogram | Measures the round-trip duration of each downstream GraphQL operation. | `operation.id`, `operation.type`, `success` (bool) |

In addition to these custom metrics, the server also emits standard [HTTP server metrics](https://opentelemetry.io/docs/specs/semconv/http/http-metrics/) (e.g., `http.server.duration`, `http.server.active_requests`) courtesy of the `axum-otel-metrics` library.


## Emitted Traces

Spans are generated for the following actions:

*   **Incoming HTTP Requests**: A root span is created for every HTTP request to the MCP server.
*   **MCP Handler Methods**: Nested spans are created for each of the main MCP protocol methods (`initialize`, `call_tool`, `list_tools`).
*   **Tool Execution**: `call_tool` spans contain nested spans for the specific tool being executed (e.g., `introspect`, `search`, or a custom GraphQL operation).
*   **Downstream GraphQL Calls**: The `execute` tool and custom operation tools create child spans for their outgoing `reqwest` HTTP calls, capturing the duration of the downstream request. The `traceparent` and `tracestate` headers are propagated automatically, enabling distributed traces.
